# -*- coding: utf-8 -*-
"""RISS CRALWERgood.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YL97fhppv_Cd_m6iXEYBRayR2jeN2-bt
"""

!pip install selenium
!apt-get update
!apt install chromium-chromedriver

!cp /usr/lib/chromium-browser/chromedriver /usr/bin

#Step 1. 필요한 모듈을 로딩합니다
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.service import Service
import time          
from urllib.request import urlopen
from bs4 import BeautifulSoup as bs
from urllib.parse import quote_plus

#Step 2. 사용자에게 검색 관련 정보들을 입력 받습니다.
print("=" *100)
print(" 이 크롤러는 RISS 사이트의 논문 및 학술자료 수집용 웹크롤러입니다.")
print("=" *100)
query_txt = input('1.수집할 자료의 키워드는 무엇입니까? : ')

#Step 3. 수집된 데이터를 저장할 파일 이름 입력받기 
ft_name = 'C:\\Users\\sjbss\\Desktop\\web\\riss.txt'
fc_name = 'C:\\Users\\sjbss\\Desktop\\web\\riss.csv'
fx_name = 'C:\\Users\\sjbss\\Desktop\\web\\riss.xls'

options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
driver = webdriver.Chrome('chromedriver', options=options)

url = 'https://www.riss.kr/'
driver.get(url)
time.sleep(5)
driver.maximize_window()

element = driver.find_element(By.ID,'query')
driver.find_element(By.ID,'query').click( )
element.send_keys(query_txt)
element.send_keys("\n")
time.sleep(2)

menu=int(2)

menu_dict = {2:'국내학술논문'}
driver.find_element(By.LINK_TEXT, menu_dict[menu]).click()
time.sleep(2)

from bs4 import BeautifulSoup
html_1 = driver.page_source
soup_1 = BeautifulSoup(html_1, 'html.parser')

if menu == 2:
    total_1 = soup_1.find('div','searchBox')
    j_name = '국내학술논문'

total_2 = total_1.select('dl > dd > span')
total_3 = total_2[0].find('span','num').get_text()
total = total_3.replace(",","")
time.sleep(1)

import math
print('검색하신 키워드 %s (으)로 총 %s 건이 검색되었습니다' %(query_txt,total))
collect_cnt = int(input('이 중에서 몇 건을 수집하시겠습니까?: '))
collect_page_cnt = math.ceil(collect_cnt / 10)
print('%s 건의 데이터를 수집하기 위해 %s 페이지의 게시물을 조회합니다.' %(collect_cnt,collect_page_cnt))
print('=' *80)

#Step 9. 각 항목별로 데이터를 추출하여 리스트에 저장하기
no2 = [ ]        #번호 저장
title2 = [ ]     #논문제목 저장
writer2 = [ ]    #논문저자 저장
org2 = [ ]       #소속기관 저장
year2 = [ ]      #발표년도
paper2 = [ ]       #논문집/자료집
pa_url2 = [ ]    #논문 URL정보
paper_cnt = range(1,11)
no = 1

import os

currentpath = os.getcwd()
print(currentpath)

for a in range(1, collect_page_cnt + 1) :

    html_2 = driver.page_source
    soup_2 = BeautifulSoup(html_2, 'html.parser')

    content_2 = soup_2.find('div','srchResultListW').find_all('li')
    cnt = 0
    
    for b in content_2 :    
        #1. 논문제목 있을 경우만
        try :
            title = b.find('div','cont').find('p','title').get_text()
        except :
            continue
        else :
            f = open(ft_name,'a', encoding="UTF-8")
            print('1.번호:',no)
            no2.append(no)
            f.write('\n'+'1.번호:' + str(no))

            print('2.논문제목:',title)
            title2.append(title)
            f.write('\n' + '2.논문제목:' + title)
            
            try:
                writer = b.find('span','writer').get_text()
            except:
                writer = "No Writer"
            print('3.저자:',writer)
            writer2.append(writer)
            f.write('\n' + '3.저자:' + writer)

            try:
                org = b.find('span','assigned').get_text()
            except:
                org = "No Org"
            print('4.소속기관:' , org)
            org2.append(org)
            f.write('\n' + '4.소속기관:' + org + '\n')
            
            etc = b.select('p > span')
            try:
                year = etc[2].get_text()
            except:
                year = "No Year"
            print('5.발표년도:' , year)
            year2.append(year)
            f.write('\n' + '5.발표년도:' + year + '\n')
            
            try:
                paper = etc[3].get_text()
            except:
                paper = "논문집 없음"
            print('6.논문집/자료집 or 학위:' , paper)
            paper2.append(paper)
            f.write('\n' + '6.논문집/자료집 or 학위:' + paper + '\n')
            
            pa_url = b.find('div', 'cont').find('p', 'title').find('a')['href']
            pa_url = 'http://www.riss.kr'+pa_url
            print('7.논문 URL 주소:' , pa_url)
            pa_url2.append(pa_url)
            f.write('\n' + '7.논문 URL 주소:' + pa_url + '\n')
            
            f.close( )
            
            no += 1
            print("\n")
            
            if no > collect_cnt :
                break

            time.sleep(1)        # 페이지 변경 전 1초 대기 

    a += 1 
    b = str(a)

    try :
        driver.find_element(By.LINK_TEXT ,'%s' %b).click() 
    except :
        driver.find_element(By.LINK_TEXT, '다음 페이지로').click()
        
print("요청하신 작업이 모두 완료되었습니다")

import pandas as pd 

df = pd.DataFrame()
df['번호']=no2
df['제목']=pd.Series(title2)
df['저자']=pd.Series(writer2)
df['소속(발행)기관']=pd.Series(org2)
df['발표년도']=pd.Series(year2)
df['논문집/자료집 or 학위']=pd.Series(paper2)
df['논문URL']=pd.Series(pa_url2)
# xls 형태로 저장하기
df.to_excel(fx_name,index=False, encoding="utf-8" , engine='openpyxl')

# csv 형태로 저장하기
df.to_csv(fc_name,index=False, encoding="utf-8-sig")

print('요청하신 데이터 수집 작업이 정상적으로 완료되었습니다')

